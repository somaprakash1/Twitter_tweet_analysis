import org.apache.spark._
import org.apache.spark.SparkContext._
import org.apache.spark.streaming._
import org.apache.spark.streaming.twitter._
import org.apache.spark.streaming.twitter.TwitterUtils
import org.apache.spark.streaming.StreamingContext._
import org.apache.log4j.Level

import twitter4j.Twitter

import scala.collection.mutable
import scala.io.Source
import org.apache.spark.sql.DataFrame

import org.apache.spark.SparkContext._
import org.apache.spark.sql.SQLContext._
import org.apache.spark.sql.SQLImplicits
import org.apache.spark.sql.SQLContext
import org.apache.log4j.lf5.LogLevel



import org.joda.time.{DateTime,Days}
import org.apache.spark.api.java.StorageLevels

object NaiveBayes {
  val posWords = Source.fromFile("H:/Projects/SparkStreamingDemo/src/main/resources/pos-words.txt").getLines()
  val negWords = Source.fromFile("H:/Projects/SparkStreamingDemo/src/main/resources/neg-words.txt").getLines()
  val stopWords = Source.fromFile("H:/Projects/SparkStreamingDemo/src/main/resources/stop-words.txt").getLines()
  
  val posWordsArr = mutable.MutableList("")
  val negWordsArr = mutable.MutableList("")
  
  for (posword <- posWords){
    posWordsArr += (posword)
  }
  
   for(negWord <-negWords){
    negWordsArr +=(negWord)
  }
   
    def findTweetSentiment(tweet:String):String = {
    var count = 0
    for(w <- tweet.split(" ")){
      //positive words
      for(positiveW <- posWordsArr){
        if(w != " " && positiveW.toString.toLowerCase()==w){
          count = count+1
        }
      }
      //negative words
      for(negativeW <- negWordsArr){
        if(w != " " && negativeW.toString.toLowerCase()==w){
          count = count-1
        }
      }
      
    }
    if(count > 0){
      // 1 - positive
      return "1"
    }
    else if(count < 0){
      // 0 - negative
      return "0"
    }
    else { 
     // 2 - Neutral
        return "2" 
     }
  }
    
  def onlyWords(text: String) : String = {
       text.split(" ").filter(_.matches("^[a-zA-Z0-9 ]+$")).fold("")((a,b) => a + " " + b).trim 
   }
  
    def main(args: Array[String]) {
    val filters : Array[String] =  Array("iphone8","Sasikala","Rahul Gandhi")
    System.setProperty("twitter4j.oauth.consumerKey", "XXXXXXXXXXXX")
    System.setProperty("twitter4j.oauth.consumerSecret", "XXXXXXXXXXXX")
    System.setProperty("twitter4j.oauth.accessToken", "XXXXXXXXXXXX")
    System.setProperty("twitter4j.oauth.accessTokenSecret", "XXXXXXXXXXXX")
     val conf = new SparkConf().setAppName("Twitter Sentiment Analyzer").setMaster("local[*]")
	                            .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
	  val sc = new SparkContext(conf)
    sc.setLogLevel("WARN")
    val ssc = new StreamingContext(sc, Seconds(100))
    val tweets = TwitterUtils.createStream(ssc,None,filters,StorageLevels.MEMORY_AND_DISK)
                             .filter { x => x.getLang == "en" }
    
     val data = tweets.map { status => 
       val sentiment = findTweetSentiment(status.getText().toLowerCase())
       (onlyWords(status.getText), sentiment.toDouble)
       
     }
    data.print(10)
   val sqlContext = new SQLContext(sc)
   import sqlContext.implicits._
    
   import org.apache.spark.ml.feature.{HashingTF, StopWordsRemover, IDF, Tokenizer}
   import org.apache.spark.ml.PipelineStage
   import org.apache.spark.sql.Row
   import org.apache.spark.ml.classification.LogisticRegression
   import org.apache.spark.ml.{Pipeline, PipelineModel}
   import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
   import org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}
   import org.apache.spark.mllib.linalg.Vector
   val url = "jdbc:mysql://10.97.15.164:3306/test_movies"
   val prop = new java.util.Properties
   prop.setProperty("user", "pavan")
   prop.setProperty("password", "pavan")
    data.foreachRDD { rdd =>
       val df = rdd.toDF("text","label")
      //df.write.jdbc(url, "senti_spark",prop)
        df.registerTempTable("STable")
        sqlContext.sql("SELECT text,label FROM STable").show()
        val splits = df.randomSplit(Array(0.6, 0.4), seed = 12345L)
        val training = splits(0).cache()
        val testing = splits(1)
         import org.apache.spark.ml.classification.NaiveBayes
         import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator
         println("Total Document Count = " + df.count())
         println("Training Count = " + training.count() + ", " + training.count*100/(df.count()).toDouble + "%")
         println("Test Count = " + testing.count() + ", " + testing.count*100/(df.count().toDouble) + "%")
         val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words")
         val remover = new StopWordsRemover().setInputCol("words").setOutputCol("filtered").setCaseSensitive(false)
         val hashingTF = new HashingTF().setNumFeatures(20000).setInputCol("filtered").setOutputCol("rawFeatures")
         val idf = new IDF().setInputCol("rawFeatures").setOutputCol("features").setMinDocFreq(0)
         val nb = new NaiveBayes() 
         val pipeline = new Pipeline().setStages(Array(tokenizer, remover, hashingTF, idf, nb))
         val model = pipeline.fit(training)
         val predictions = model.transform(testing)
         predictions.select("text", "probability", "label","prediction").show(5)
         
    }
  
    ssc.checkpoint("H:/Projects/SparkStreamingDemo/src/main/resources/")
    ssc.start()
    ssc.awaitTermination()
    }
